{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape is  (10886, 12)  and test.shape is  (6493, 9)\n",
      "traind.shape is  (10886, 9)  and testd.shape is  (6493, 9)\n"
     ]
    }
   ],
   "source": [
    "#This code run gradient descent on the kaggle RMSLE loss function with L2 Regularization\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "dateparse=lambda x:pd.datetime.strptime(x,'%Y-%m-%d %H:%M:%S')\n",
    "train=pd.read_csv('train.csv',parse_dates=['datetime'],date_parser=dateparse)\n",
    "test=pd.read_csv('test.csv',parse_dates=['datetime'],date_parser=dateparse)\n",
    "\n",
    "#This made very little difference for the kaggle loss function using gradient decent\n",
    "\n",
    "#test['windspeed']=np.log(test['windspeed']+1)\n",
    "#train['windspeed']=np.log(train['windspeed']+1)\n",
    "print 'train.shape is ',train.shape,' and test.shape is ',test.shape\n",
    "\n",
    "\n",
    "def extractFeaturesTrain(data):\n",
    "    #print 'data is ',data\n",
    "    data['Hour']=data.datetime.dt.hour\n",
    "    labels=data['count']\n",
    "    train_years=data.datetime.dt.year\n",
    "    train_months=data.datetime.dt.month\n",
    "    data=data.drop(['datetime','count','casual','registered'], axis = 1)\n",
    "    \n",
    "    return np.array(data),np.array(labels),np.array(train_years),np.array(train_months),(data.columns.values)\n",
    "\n",
    "def extractFeaturesTest(data):\n",
    "    #print 'data is \\n',data\n",
    "    data['Hour']=data.datetime.dt.hour\n",
    "    test_years=data.datetime.dt.year\n",
    "    test_months=data.datetime.dt.month\n",
    "    data=data.drop(['datetime'], axis = 1)\n",
    "    return np.array(data),np.array(test_years),np.array(test_months)\n",
    "    \n",
    "train2=copy(train)\n",
    "test2=copy(test)\n",
    "test=np.array(test)\n",
    "#print 'train2 is ',train2\n",
    "traind,labelsTrain,train_years,train_months,headers=extractFeaturesTrain(train2)\n",
    "testd,test_years,test_months=extractFeaturesTest(test2)\n",
    "print 'traind.shape is ',traind.shape,' and testd.shape is ',testd.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10886, 40)\n",
      "(6493, 40)\n",
      "(10886, 41)\n",
      "(6493, 41)\n"
     ]
    }
   ],
   "source": [
    "enc=OneHotEncoder(categorical_features=[0,1,2,3,8],sparse=False)\n",
    "traind2=enc.fit_transform(traind)\n",
    "print traind2.shape\n",
    "testd2=enc.fit_transform(testd)\n",
    "print testd2.shape\n",
    "ones1=np.ones((traind.shape[0],1))\n",
    "ones2=np.ones((testd.shape[0],1))\n",
    "traind2=copy(np.hstack((traind2,ones1)))\n",
    "testd2=copy(np.hstack((testd2,ones2)))\n",
    "print traind2.shape\n",
    "print testd2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in getSplits ,train is \n",
      "[[Timestamp('2011-01-01 00:00:00') 1 0 ..., 13 16 0]\n",
      " [Timestamp('2011-01-01 01:00:00') 1 0 ..., 32 40 1]\n",
      " [Timestamp('2011-01-01 02:00:00') 1 0 ..., 27 32 2]\n",
      " ..., \n",
      " [Timestamp('2012-12-19 21:00:00') 4 0 ..., 164 168 21]\n",
      " [Timestamp('2012-12-19 22:00:00') 4 0 ..., 117 129 22]\n",
      " [Timestamp('2012-12-19 23:00:00') 4 0 ..., 84 88 23]]\n",
      "trainSubset min is   2011-01-01 00:00:00  and max is  2011-11-19 23:00:00\n",
      "testSubset  min is   2011-12-01 00:00:00  and max is  2011-12-19 23:00:00\n",
      "weights are \n",
      "(1, 41)\n",
      "For regularization factor  0.0\n",
      "final training loss is  1.36496093134\n",
      "For regularization factor  0.1\n",
      "final training loss is  2.24331121767\n",
      "For regularization factor  0.2\n",
      "final training loss is  2.45930489673\n",
      "For regularization factor  0.3\n",
      "final training loss is  2.59113775189\n",
      "For regularization factor  0.4\n",
      "final training loss is  2.68654447052\n",
      "For regularization factor  0.5\n",
      "final training loss is  2.76139717248\n",
      "For regularization factor  0.6\n",
      "final training loss is  2.8229991281\n",
      "For regularization factor  0.7\n",
      "final training loss is  2.87533170724\n",
      "For regularization factor  0.8\n",
      "final training loss is  2.92080960883\n",
      "For regularization factor  0.9\n",
      "final training loss is  2.96100970462\n",
      "For regularization factor  1.0\n",
      "final training loss is  2.99701939817\n",
      "For regularization factor  1.1\n",
      "final training loss is  3.02962065707\n",
      "For regularization factor  1.2\n",
      "final training loss is  3.05939484218\n",
      "For regularization factor  1.3\n",
      "final training loss is  3.08678604799\n",
      "For regularization factor  1.4\n",
      "final training loss is  3.1121412365\n",
      "For regularization factor  1.5\n",
      "final training loss is  3.13573668248\n",
      "For regularization factor  1.6\n",
      "final training loss is  3.15779598073\n",
      "For regularization factor  1.7\n",
      "final training loss is  3.17850265585\n",
      "For regularization factor  1.8\n",
      "final training loss is  3.19800920779\n",
      "For regularization factor  1.9\n",
      "final training loss is  3.2164437387\n",
      "Number of Negative values predicted are  0\n",
      "loss with year = 2011  and month =  12  is  [ 2.73863954]\n",
      "trainSubset min is   2011-01-01 00:00:00  and max is  2012-11-19 23:00:00\n",
      "testSubset  min is   2012-12-01 00:00:00  and max is  2012-12-19 23:00:00\n",
      "weights are \n",
      "(1, 41)\n",
      "For regularization factor  0.0\n",
      "final training loss is  1.44264136432\n",
      "For regularization factor  0.1\n",
      "final training loss is  2.43784771087\n",
      "For regularization factor  0.2\n",
      "final training loss is  2.66352617924\n",
      "For regularization factor  0.3\n",
      "final training loss is  2.80019313493\n",
      "For regularization factor  0.4\n",
      "final training loss is  2.89871218921\n",
      "For regularization factor  0.5\n",
      "final training loss is  2.97581685078\n",
      "For regularization factor  0.6\n",
      "final training loss is  3.03916257802\n",
      "For regularization factor  0.7\n",
      "final training loss is  3.09290681143\n",
      "For regularization factor  0.8\n",
      "final training loss is  3.13956399085\n",
      "For regularization factor  0.9\n",
      "final training loss is  3.18077256284\n",
      "For regularization factor  1.0\n",
      "final training loss is  3.21766041795\n",
      "For regularization factor  1.1\n",
      "final training loss is  3.25103746293\n",
      "For regularization factor  1.2\n",
      "final training loss is  3.28150509082\n",
      "For regularization factor  1.3\n",
      "final training loss is  3.30952221225\n",
      "For regularization factor  1.4\n",
      "final training loss is  3.3354470334\n",
      "For regularization factor  1.5\n",
      "final training loss is  3.35956454944\n",
      "For regularization factor  1.6\n",
      "final training loss is  3.38210524481\n",
      "For regularization factor  1.7\n",
      "final training loss is  3.40325817513\n",
      "For regularization factor  1.8\n",
      "final training loss is  3.4231803432\n",
      "For regularization factor  1.9\n",
      "final training loss is  3.44200356238\n",
      "Number of Negative values predicted are  0\n",
      "loss with year = 2012  and month =  12  is  [ 2.48524151]\n"
     ]
    }
   ],
   "source": [
    "train=np.array(train)\n",
    "\n",
    "def getSplits(years,months):\n",
    "    locsTrain=[]\n",
    "    locsTest=[]\n",
    "    print 'in getSplits ,train is \\n',train\n",
    "    for i in range(0,train.shape[0]):\n",
    "            if (train[i,0].year==years[0] or train[i,0].year==years[1]) and (train[i,0].month in months):\n",
    "                locsTest.append(i)\n",
    "            else:\n",
    "                locsTrain.append(i) \n",
    "    \n",
    "    return locsTrain,locsTest\n",
    "\n",
    "def getCustomLocsTest(year,month,data):\n",
    "    locs=[]\n",
    "    for i in range(0,data.shape[0]):\n",
    "        if data[i][0].year==year and data[i][0].month==month:\n",
    "            locs.append(i)\n",
    "    return locs\n",
    "\n",
    "def replaceNegaticeValuesWithZeroAndCountThem(ypred):\n",
    "    count=0\n",
    "    for i in range(ypred.shape[0]):\n",
    "        if(ypred[i]<0):\n",
    "            ypred[i]=0\n",
    "            count+=1\n",
    "    #print 'Number of Negative values predicted are ',count\n",
    "    return ypred,count\n",
    "\n",
    "def calculateGradientAndLoss(weights,x,y,year,month):\n",
    "    y2=y.reshape(-1,1)\n",
    "    weights=weights.reshape(1,-1)\n",
    "    print 'weights are \\n',weights.shape\n",
    "    #print 'y2 is ',y2.shape\n",
    "    losses=[]\n",
    "    lambdas=np.arange(0.0,2,0.1)\n",
    "    #lambdas=[0.0]\n",
    "    minLoss=9999999\n",
    "    learning_rate=0.4\n",
    "    fig=plt.figure()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    fig.suptitle('Gradient Descent on Cross Validation for year  '+str(year)+' with test month as '+str(month)+' Learning rate of '+str(learning_rate).split('.')[0]+'_'+str(learning_rate).split('.')[1])\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.text(0.5,0.04,'-------------------------------------------------------------------Iterations---------------------------------------------------> ',ha='center')\n",
    "    fig.text(0.08,0.5,'----------------------------------------------------Kaggle Loss function--------------------------------------------------------> ',va='center',rotation='vertical')\n",
    "    counter=1\n",
    "    for lambda1 in lambdas:\n",
    "        weights=np.random.rand(1,x.shape[1])\n",
    "        print 'For regularization factor ',lambda1\n",
    "        losses=[]\n",
    "        for i in range(1000):\n",
    "            h=np.dot(x,weights.T)\n",
    "            #print 'h is \\n',h\n",
    "            h,count=replaceNegaticeValuesWithZeroAndCountThem(h)\n",
    "            err=np.log(y2+1)-np.log(h+1)\n",
    "            #print 'err is ',err.shape\n",
    "            loss=np.sum(err**2)\n",
    "            loss=np.sqrt(loss/x.shape[0])\n",
    "            grad1=(err)/(h+1)\n",
    "            #print 'grad1 is ',grad1.shape\n",
    "            grad=( 2*np.dot(grad1.T,x)/x.shape[0] ) - lambda1*weights\n",
    "            #print 'Loss at iteration ',i,' is ',loss\n",
    "            losses.append(loss)\n",
    "            #print 'grad is ',grad.shape\n",
    "            weights=weights+learning_rate*grad \n",
    "        if loss<minLoss:\n",
    "            minLoss=loss\n",
    "            weightsRes=weights\n",
    "                \n",
    "    \n",
    "        print 'final training loss is ',loss\n",
    "        #plt.figure().set_size_inches(18.5, 10.5)\n",
    "        #plt.plot(losses)\n",
    "        \n",
    "        title= ' Regularization of '+ str(lambda1).split('.')[0]+'_'+str(lambda1).split('.')[1]\n",
    "       \n",
    "        ax=fig.add_subplot(4,5,counter)\n",
    "        counter=counter+1\n",
    "        ax.plot(losses)\n",
    "        #print 'title is ',title\n",
    "        ax.set_title(title)\n",
    "        #plt.xlabel('Iterations')\n",
    "        #plt.ylabel('Kaggle  Loss functions')\n",
    "    plt.savefig('Gradient Descent on Cross Validation for year  '+str(year)+' with test month as '+str(month)+' Learningrate of '+str(learning_rate).split('.')[0]+'_'+str(learning_rate).split('.')[1])\n",
    "    plt.show()\n",
    "    return weightsRes\n",
    "    \n",
    "def TrainFucntion(x,y,year,month):\n",
    "    weights=np.random.rand(1,x.shape[1])\n",
    "    for i in range(x.shape[1]):\n",
    "        max1=max(x[:,i])\n",
    "        if max1!=0:\n",
    "            x[:,i]=x[:,i]/max1\n",
    "        \n",
    "    weights=calculateGradientAndLoss(weights,x,y,year,month)\n",
    "    return weights\n",
    "    \n",
    "\n",
    "def Predict(weights,test):\n",
    "    \n",
    "    return np.dot(test,weights.T)\n",
    "\n",
    "def findLoss(gold,predicted):\n",
    "    loss=0\n",
    "    \n",
    "    #print 'predicted is ',predicted\n",
    "    for i in range(gold.shape[0]):\n",
    "        loss+=(np.log(predicted[i]+1) -np.log(gold[i]+1))**2\n",
    "    \n",
    "    loss=loss/gold.shape[0]\n",
    "    return np.sqrt(loss)\n",
    "\n",
    "def crossValidate():\n",
    "        months=[12]\n",
    "        locsTrain,locsTest=getSplits([2011,2012],months)\n",
    "        \n",
    "        testSubset=traind2[locsTest]\n",
    "        testSubset2=train[locsTest]\n",
    "        testLabels=labelsTrain[locsTest]\n",
    "        #rf3=RandomForestRegressor(20) \n",
    "        \n",
    "        trainSubset=traind2[locsTrain]\n",
    "        trainSubset2=train[locsTrain]\n",
    "        trainLabels=labelsTrain[locsTrain]\n",
    "        \n",
    "        for i in [2011,2012]:\n",
    "            for j in months:\n",
    "                testLocs=getCustomLocsTest(i,j,testSubset2)\n",
    "                testSubset3=testSubset2[testLocs]\n",
    "                testSubset4=testSubset[testLocs]\n",
    "                testLabels4=testLabels[testLocs]\n",
    "                \n",
    "                trainLocs2=np.where(trainSubset2[:,0]<=min(testSubset3[:,0]))\n",
    "                \n",
    "                trainSubset3=trainSubset[trainLocs2]\n",
    "                trainLabels3=trainLabels[trainLocs2]\n",
    "                x1=trainSubset2[trainLocs2]\n",
    "                x2=testSubset2[testLocs]\n",
    "                \n",
    "                print 'trainSubset min is  ', min(x1[:,0]),' and max is ',max(x1[:,0])\n",
    "                print 'testSubset  min is  ', min(x2[:,0]),' and max is ',max(x2[:,0])\n",
    "                \n",
    "                #rf3.fit(trainSubset3,trainLabels3)change here to program new function to train\n",
    "                weights=TrainFucntion(trainSubset3,trainLabels3,i,j)\n",
    "                \n",
    "                ypred=Predict(weights,testSubset4)\n",
    "                ypred,count=replaceNegaticeValuesWithZeroAndCountThem(ypred)\n",
    "                print 'Number of Negative values predicted are ',count\n",
    "                print 'loss with year =',i,' and month = ',j,' is ',findLoss(testLabels4,ypred)\n",
    "                \n",
    "crossValidate() \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
